WARNING:tensorflow:Deprecation warnings have been disabled. Set TF_ENABLE_DEPRECATION_WARNINGS=1 to re-enable them.
Extracting /work/gitlab/tensorrt-cookbook-in-chinese/04-Parser/TensorFlow-Caffe-TensorRT/../../00-MNISTData/train-images-idx3-ubyte.gz
Extracting /work/gitlab/tensorrt-cookbook-in-chinese/04-Parser/TensorFlow-Caffe-TensorRT/../../00-MNISTData/train-labels-idx1-ubyte.gz
Extracting /work/gitlab/tensorrt-cookbook-in-chinese/04-Parser/TensorFlow-Caffe-TensorRT/../../00-MNISTData/t10k-images-idx3-ubyte.gz
Extracting /work/gitlab/tensorrt-cookbook-in-chinese/04-Parser/TensorFlow-Caffe-TensorRT/../../00-MNISTData/t10k-labels-idx1-ubyte.gz
2022-05-29 07:13:18.402717, step 0, acc = 0.070312
2022-05-29 07:13:24.851519, step 100, acc = 0.914062
2022-05-29 07:13:25.344739, step 200, acc = 0.937500
2022-05-29 07:13:25.834717, step 300, acc = 0.937500
2022-05-29 07:13:26.325776, step 400, acc = 0.945312
2022-05-29 07:13:26.816906, step 500, acc = 0.960938
2022-05-29 07:13:27.308510, step 600, acc = 0.984375
2022-05-29 07:13:27.798331, step 700, acc = 0.960938
2022-05-29 07:13:28.288092, step 800, acc = 0.960938
2022-05-29 07:13:28.779139, step 900, acc = 0.984375
2022-05-29 07:13:29.265402, test acc = 0.990000
Succeeded building model in TensorFlow!

WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.
WARNING:tensorflow:From /home/wili/software/anaconda3/envs/caffe/lib/python3.7/site-packages/mmdnn/conversion/tensorflow/tensorflow_parser.py:114: The name tf.train.NewCheckpointReader is deprecated. Please use tf.compat.v1.train.NewCheckpointReader instead.

WARNING:tensorflow:From /home/wili/software/anaconda3/envs/caffe/lib/python3.7/site-packages/mmdnn/conversion/tensorflow/tensorflow_parser.py:114: The name tf.train.NewCheckpointReader is deprecated. Please use tf.compat.v1.train.NewCheckpointReader instead.

WARNING:tensorflow:From /home/wili/software/anaconda3/envs/caffe/lib/python3.7/site-packages/mmdnn/conversion/tensorflow/tensorflow_parser.py:269: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.extract_sub_graph`
WARNING:tensorflow:From /home/wili/software/anaconda3/envs/caffe/lib/python3.7/site-packages/mmdnn/conversion/tensorflow/tensorflow_parser.py:269: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.extract_sub_graph`
2022-05-29 15:19:14.524305: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fold_constants
WARNING:tensorflow:From /home/wili/software/anaconda3/envs/caffe/lib/python3.7/site-packages/mmdnn/conversion/tensorflow/tensorflow_parser.py:305: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /home/wili/software/anaconda3/envs/caffe/lib/python3.7/site-packages/mmdnn/conversion/tensorflow/tensorflow_parser.py:305: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /home/wili/software/anaconda3/envs/caffe/lib/python3.7/site-packages/mmdnn/conversion/tensorflow/tensorflow_parser.py:310: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

WARNING:tensorflow:From /home/wili/software/anaconda3/envs/caffe/lib/python3.7/site-packages/mmdnn/conversion/tensorflow/tensorflow_parser.py:310: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

2022-05-29 15:19:14.528325: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2022-05-29 15:19:14.555082: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-05-29 15:19:14.555301: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: 
name: NVIDIA GeForce GTX 1070 major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:01:00.0
2022-05-29 15:19:14.555416: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.0'; dlerror: libcudart.so.10.0: cannot open shared object file: No such file or directory
2022-05-29 15:19:14.555483: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcublas.so.10.0'; dlerror: libcublas.so.10.0: cannot open shared object file: No such file or directory
2022-05-29 15:19:14.555537: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcufft.so.10.0'; dlerror: libcufft.so.10.0: cannot open shared object file: No such file or directory
2022-05-29 15:19:14.555607: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcurand.so.10.0'; dlerror: libcurand.so.10.0: cannot open shared object file: No such file or directory
2022-05-29 15:19:14.555648: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusolver.so.10.0'; dlerror: libcusolver.so.10.0: cannot open shared object file: No such file or directory
2022-05-29 15:19:14.555685: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusparse.so.10.0'; dlerror: libcusparse.so.10.0: cannot open shared object file: No such file or directory
2022-05-29 15:19:14.555722: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory
2022-05-29 15:19:14.555733: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1662] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2022-05-29 15:19:14.555920: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2022-05-29 15:19:14.580226: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 4200000000 Hz
2022-05-29 15:19:14.580557: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55cc52ce2af0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2022-05-29 15:19:14.580591: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2022-05-29 15:19:14.638114: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-05-29 15:19:14.638312: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55cc5377d930 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-05-29 15:19:14.638334: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1070, Compute Capability 6.1
2022-05-29 15:19:14.638396: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-05-29 15:19:14.638407: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      
WARNING:tensorflow:From /home/wili/software/anaconda3/envs/caffe/lib/python3.7/site-packages/mmdnn/conversion/tensorflow/tensorflow_parser.py:312: The name tf.train.export_meta_graph is deprecated. Please use tf.compat.v1.train.export_meta_graph instead.

WARNING:tensorflow:From /home/wili/software/anaconda3/envs/caffe/lib/python3.7/site-packages/mmdnn/conversion/tensorflow/tensorflow_parser.py:312: The name tf.train.export_meta_graph is deprecated. Please use tf.compat.v1.train.export_meta_graph instead.

WARNING: Logging before InitGoogleLogging() is written to STDERR
I0529 15:19:15.278082 72078 net.cpp:51] Initializing net from parameters: 
state {
  phase: TRAIN
  level: 0
}
layer {
  name: "Placeholder"
  type: "Input"
  top: "Placeholder"
  input_param {
    shape {
      dim: 1
      dim: 1
      dim: 28
      dim: 28
    }
  }
}
layer {
  name: "Conv2D"
  type: "Convolution"
  bottom: "Placeholder"
  top: "Conv2D"
  convolution_param {
    num_output: 32
    bias_term: false
    group: 1
    stride: 1
    pad_h: 2
    pad_w: 2
    kernel_h: 5
    kernel_w: 5
  }
}
layer {
  name: "Relu"
  type: "ReLU"
  bottom: "Conv2D"
  top: "Conv2D"
}
layer {
  name: "MaxPool2d"
  type: "Pooling"
  bottom: "Conv2D"
  top: "MaxPool2d"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
    pad_h: 0
    pad_w: 0
  }
}
layer {
  name: "Conv2D_1"
  type: "Convolution"
  bottom: "MaxPool2d"
  top: "Conv2D_1"
  convolution_param {
    num_output: 64
    bias_term: false
    group: 1
    stride: 1
    pad_h: 2
    pad_w: 2
    kernel_h: 5
    kernel_w: 5
  }
}
layer {
  name: "Relu_1"
  type: "ReLU"
  bottom: "Conv2D_1"
  top: "Conv2D_1"
}
layer {
  name: "MaxPool2d_1"
  type: "Pooling"
  bottom: "Conv2D_1"
  top: "MaxPool2d_1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
    pad_h: 0
    pad_w: 0
  }
}
layer {
  name: "Reshape"
  type: "Reshape"
  bottom: "MaxPool2d_1"
  top: "Reshape"
  reshape_param {
    shape {
      dim: 1
      dim: 3136
    }
  }
}
layer {
  name: "MatMul"
  type: "InnerProduct"
  bottom: "Reshape"
  top: "MatMul"
  inner_product_param {
    num_output: 1024
    bias_term: false
  }
}
layer {
  name: "Relu_2"
  type: "ReLU"
  bottom: "MatMul"
  top: "MatMul"
}
layer {
  name: "MatMul_1"
  type: "InnerProduct"
  bottom: "MatMul"
  top: "MatMul_1"
  inner_product_param {
    num_output: 10
    bias_term: false
  }
}
layer {
  name: "y"
  type: "Softmax"
  bottom: "MatMul_1"
  top: "y"
}
I0529 15:19:15.278178 72078 layer_factory.hpp:77] Creating layer Placeholder
I0529 15:19:15.278192 72078 net.cpp:84] Creating Layer Placeholder
I0529 15:19:15.278198 72078 net.cpp:380] Placeholder -> Placeholder
I0529 15:19:15.278224 72078 net.cpp:122] Setting up Placeholder
I0529 15:19:15.278231 72078 net.cpp:129] Top shape: 1 1 28 28 (784)
I0529 15:19:15.278237 72078 net.cpp:137] Memory required for data: 3136
I0529 15:19:15.278241 72078 layer_factory.hpp:77] Creating layer Conv2D
I0529 15:19:15.278249 72078 net.cpp:84] Creating Layer Conv2D
I0529 15:19:15.278254 72078 net.cpp:406] Conv2D <- Placeholder
I0529 15:19:15.278259 72078 net.cpp:380] Conv2D -> Conv2D
I0529 15:19:15.278275 72078 net.cpp:122] Setting up Conv2D
I0529 15:19:15.278281 72078 net.cpp:129] Top shape: 1 32 28 28 (25088)
I0529 15:19:15.278287 72078 net.cpp:137] Memory required for data: 103488
I0529 15:19:15.278295 72078 layer_factory.hpp:77] Creating layer Relu
I0529 15:19:15.278302 72078 net.cpp:84] Creating Layer Relu
I0529 15:19:15.278307 72078 net.cpp:406] Relu <- Conv2D
I0529 15:19:15.278313 72078 net.cpp:367] Relu -> Conv2D (in-place)
I0529 15:19:15.278321 72078 net.cpp:122] Setting up Relu
I0529 15:19:15.278326 72078 net.cpp:129] Top shape: 1 32 28 28 (25088)
I0529 15:19:15.278331 72078 net.cpp:137] Memory required for data: 203840
I0529 15:19:15.278335 72078 layer_factory.hpp:77] Creating layer MaxPool2d
I0529 15:19:15.278340 72078 net.cpp:84] Creating Layer MaxPool2d
I0529 15:19:15.278345 72078 net.cpp:406] MaxPool2d <- Conv2D
I0529 15:19:15.278350 72078 net.cpp:380] MaxPool2d -> MaxPool2d
I0529 15:19:15.278359 72078 net.cpp:122] Setting up MaxPool2d
I0529 15:19:15.278365 72078 net.cpp:129] Top shape: 1 32 14 14 (6272)
I0529 15:19:15.278371 72078 net.cpp:137] Memory required for data: 228928
I0529 15:19:15.278375 72078 layer_factory.hpp:77] Creating layer Conv2D_1
I0529 15:19:15.278383 72078 net.cpp:84] Creating Layer Conv2D_1
I0529 15:19:15.278386 72078 net.cpp:406] Conv2D_1 <- MaxPool2d
I0529 15:19:15.278391 72078 net.cpp:380] Conv2D_1 -> Conv2D_1
I0529 15:19:15.278417 72078 net.cpp:122] Setting up Conv2D_1
I0529 15:19:15.278424 72078 net.cpp:129] Top shape: 1 64 14 14 (12544)
I0529 15:19:15.278430 72078 net.cpp:137] Memory required for data: 279104
I0529 15:19:15.278441 72078 layer_factory.hpp:77] Creating layer Relu_1
I0529 15:19:15.278448 72078 net.cpp:84] Creating Layer Relu_1
I0529 15:19:15.278455 72078 net.cpp:406] Relu_1 <- Conv2D_1
I0529 15:19:15.278460 72078 net.cpp:367] Relu_1 -> Conv2D_1 (in-place)
I0529 15:19:15.278465 72078 net.cpp:122] Setting up Relu_1
I0529 15:19:15.278470 72078 net.cpp:129] Top shape: 1 64 14 14 (12544)
I0529 15:19:15.278475 72078 net.cpp:137] Memory required for data: 329280
I0529 15:19:15.278481 72078 layer_factory.hpp:77] Creating layer MaxPool2d_1
I0529 15:19:15.278486 72078 net.cpp:84] Creating Layer MaxPool2d_1
I0529 15:19:15.278491 72078 net.cpp:406] MaxPool2d_1 <- Conv2D_1
I0529 15:19:15.278496 72078 net.cpp:380] MaxPool2d_1 -> MaxPool2d_1
I0529 15:19:15.278503 72078 net.cpp:122] Setting up MaxPool2d_1
I0529 15:19:15.278509 72078 net.cpp:129] Top shape: 1 64 7 7 (3136)
I0529 15:19:15.278515 72078 net.cpp:137] Memory required for data: 341824
I0529 15:19:15.278519 72078 layer_factory.hpp:77] Creating layer Reshape
I0529 15:19:15.278525 72078 net.cpp:84] Creating Layer Reshape
I0529 15:19:15.278532 72078 net.cpp:406] Reshape <- MaxPool2d_1
I0529 15:19:15.278537 72078 net.cpp:380] Reshape -> Reshape
I0529 15:19:15.278545 72078 net.cpp:122] Setting up Reshape
I0529 15:19:15.278551 72078 net.cpp:129] Top shape: 1 3136 (3136)
I0529 15:19:15.278558 72078 net.cpp:137] Memory required for data: 354368
I0529 15:19:15.278561 72078 layer_factory.hpp:77] Creating layer MatMul
I0529 15:19:15.278566 72078 net.cpp:84] Creating Layer MatMul
I0529 15:19:15.278573 72078 net.cpp:406] MatMul <- Reshape
I0529 15:19:15.278578 72078 net.cpp:380] MatMul -> MatMul
I0529 15:19:15.283522 72078 net.cpp:122] Setting up MatMul
I0529 15:19:15.283550 72078 net.cpp:129] Top shape: 1 1024 (1024)
I0529 15:19:15.283557 72078 net.cpp:137] Memory required for data: 358464
I0529 15:19:15.283565 72078 layer_factory.hpp:77] Creating layer Relu_2
I0529 15:19:15.283573 72078 net.cpp:84] Creating Layer Relu_2
I0529 15:19:15.283577 72078 net.cpp:406] Relu_2 <- MatMul
I0529 15:19:15.283583 72078 net.cpp:367] Relu_2 -> MatMul (in-place)
I0529 15:19:15.283591 72078 net.cpp:122] Setting up Relu_2
I0529 15:19:15.283593 72078 net.cpp:129] Top shape: 1 1024 (1024)
I0529 15:19:15.283597 72078 net.cpp:137] Memory required for data: 362560
I0529 15:19:15.283601 72078 layer_factory.hpp:77] Creating layer MatMul_1
I0529 15:19:15.283607 72078 net.cpp:84] Creating Layer MatMul_1
I0529 15:19:15.283610 72078 net.cpp:406] MatMul_1 <- MatMul
I0529 15:19:15.283615 72078 net.cpp:380] MatMul_1 -> MatMul_1
I0529 15:19:15.283627 72078 net.cpp:122] Setting up MatMul_1
I0529 15:19:15.283633 72078 net.cpp:129] Top shape: 1 10 (10)
I0529 15:19:15.283638 72078 net.cpp:137] Memory required for data: 362600
I0529 15:19:15.283641 72078 layer_factory.hpp:77] Creating layer y
I0529 15:19:15.283648 72078 net.cpp:84] Creating Layer y
I0529 15:19:15.283651 72078 net.cpp:406] y <- MatMul_1
I0529 15:19:15.283656 72078 net.cpp:380] y -> y
I0529 15:19:15.283668 72078 net.cpp:122] Setting up y
I0529 15:19:15.283674 72078 net.cpp:129] Top shape: 1 10 (10)
I0529 15:19:15.283677 72078 net.cpp:137] Memory required for data: 362640
I0529 15:19:15.283681 72078 net.cpp:200] y does not need backward computation.
I0529 15:19:15.283685 72078 net.cpp:200] MatMul_1 does not need backward computation.
I0529 15:19:15.283689 72078 net.cpp:200] Relu_2 does not need backward computation.
I0529 15:19:15.283692 72078 net.cpp:200] MatMul does not need backward computation.
I0529 15:19:15.283696 72078 net.cpp:200] Reshape does not need backward computation.
I0529 15:19:15.283700 72078 net.cpp:200] MaxPool2d_1 does not need backward computation.
I0529 15:19:15.283705 72078 net.cpp:200] Relu_1 does not need backward computation.
I0529 15:19:15.283710 72078 net.cpp:200] Conv2D_1 does not need backward computation.
I0529 15:19:15.283713 72078 net.cpp:200] MaxPool2d does not need backward computation.
I0529 15:19:15.283718 72078 net.cpp:200] Relu does not need backward computation.
I0529 15:19:15.283722 72078 net.cpp:200] Conv2D does not need backward computation.
I0529 15:19:15.283735 72078 net.cpp:200] Placeholder does not need backward computation.
I0529 15:19:15.283738 72078 net.cpp:242] This network produces output y
I0529 15:19:15.283746 72078 net.cpp:255] Network initialization done.
Parse file [./ckpt/model.ckpt.meta] with binary format successfully.
Tensorflow model file [./ckpt/model.ckpt.meta] loaded successfully.
Tensorflow checkpoint file [./ckpt/model.ckpt] loaded successfully. [18] variables loaded.
IR network structure is saved as [d6aaeb8cda734ea3867ecf9e802a9fee.json].
IR network structure is saved as [d6aaeb8cda734ea3867ecf9e802a9fee.pb].
IR weights are saved as [d6aaeb8cda734ea3867ecf9e802a9fee.npy].
Parse file [d6aaeb8cda734ea3867ecf9e802a9fee.pb] with binary format successfully.
Target network code snippet is saved as [d6aaeb8cda734ea3867ecf9e802a9fee.py].
Target weights are saved as [d6aaeb8cda734ea3867ecf9e802a9fee.npy].
Caffe model files are saved as [model.prototxt] and [model.caffemodel], generated by [d6aaeb8cda734ea3867ecf9e802a9fee.py] and [d6aaeb8cda734ea3867ecf9e802a9fee.npy].

[05/29/2022-07:31:07] [TRT] [I] [MemUsageChange] Init CUDA: CPU +171, GPU +0, now: CPU 197, GPU 571 (MiB)
[05/29/2022-07:31:07] [TRT] [I] Loaded engine size: 6 MiB
[05/29/2022-07:31:07] [TRT] [V] Using cublas as a tactic source
[05/29/2022-07:31:07] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +267, GPU +110, now: CPU 470, GPU 689 (MiB)
[05/29/2022-07:31:07] [TRT] [V] Using cuDNN as a tactic source
[05/29/2022-07:31:07] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +113, GPU +48, now: CPU 583, GPU 737 (MiB)
[05/29/2022-07:31:07] [TRT] [V] Deserialization required 315873 microseconds.
[05/29/2022-07:31:07] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +6, now: CPU 0, GPU 6 (MiB)
[05/29/2022-07:31:07] [TRT] [V] Using cublas as a tactic source
[05/29/2022-07:31:07] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +10, now: CPU 576, GPU 729 (MiB)
[05/29/2022-07:31:07] [TRT] [V] Using cuDNN as a tactic source
[05/29/2022-07:31:07] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 576, GPU 737 (MiB)
[05/29/2022-07:31:07] [TRT] [V] Total per-runner device persistent memory is 0
[05/29/2022-07:31:07] [TRT] [V] Total per-runner host persistent memory is 4448
[05/29/2022-07:31:07] [TRT] [V] Allocated activation device memory of size 125952
[05/29/2022-07:31:07] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 6 (MiB)
Succeeded loading engine!
Binding0-> (1, 1, 28, 28) (1, 1, 28, 28) DataType.FLOAT
Binding1-> (1, 1) (1, 1) DataType.INT32
inputH0 : (28, 28)
outputH0: (1, 1)
[[8]]
Succeeded running model in TensorRT!
